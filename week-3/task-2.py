# -*- coding: utf-8 -*-
"""t2w3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P_OMyySb00OUAvvQJPCKKrKPCUwICW0k
"""

import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])

train_dataset = torchvision.datasets.CIFAR10(root='./data',train=True, download=True, transform=transform)
test_dataset = torchvision.datasets.CIFAR10(root='./data',train=False, download=True, transform=transform)

input,label = train_dataset[0]
print(input.shape)
print(label)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def train_model(model, train_data, test_data, num_epochs, loss_func, optimizer, device):
  for i in range(num_epochs):
    model.train()
    tot_loss = 0
    for data in train_data:
      X, y = data
      X, y = X.to(device), y.to(device)
      optimizer.zero_grad()
      y_pred = model(X)
      loss = loss_func(y_pred, y)
      loss.backward()
      optimizer.step()
      tot_loss+=loss
    print(f"Epoch: {i+1}, Avg Loss: {tot_loss/len(train_data)}")
    model.eval()
    with torch.no_grad():
      correct = 0
      total = 0
      for data in test_data:
        X, y = data
        X, y = X.to(device), y.to(device)
        y_pred = model(X)
        total += y.size(0)
        correct += (y_pred.argmax(1)==y).sum().item()
      print(f"Accuracy: {correct/total}")

model_CIFAR = nn.Sequential(nn.Conv2d(3,6,5),nn.ReLU(),nn.MaxPool2d(2,2),
                            nn.Conv2d(6,16,5),nn.ReLU(),nn.MaxPool2d(2,2),
                            nn.Flatten(),
                            nn.Linear(16*5*5,120),nn.ReLU(),
                            nn.Linear(120,84),nn.ReLU(),
                            nn.Linear(84,10))

batch_size = 64
learning_rate = 0.001
train_batch = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle = True )
test_batch = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle = False )
loss_func = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model_CIFAR.parameters(), lr=learning_rate)
num_epochs = 10
model_CIFAR.to(device)
print("Training CIFAR10 data")
train_model(model_CIFAR, train_batch, test_batch, num_epochs, loss_func, optimizer, device)

transform_fashion = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307), (0.3081))])

train_dataset_fashion = torchvision.datasets.FashionMNIST(root='./data_fashion', train=True, download=True, transform=transform_fashion)
test_dataset_fashion = torchvision.datasets.FashionMNIST(root='./data_fashion', train=False, download=True, transform=transform_fashion)

model_fashion = nn.Sequential(nn.Conv2d(1,6,5),nn.ReLU(),nn.MaxPool2d(2,2),
                            nn.Conv2d(6,16,5),nn.ReLU(),nn.MaxPool2d(2,2),
                            nn.Flatten(),
                            nn.Linear(16*4*4,100),nn.ReLU(),
                            nn.Linear(100,25),nn.ReLU(),
                            nn.Linear(25,10))

batch_size = 64
learning_rate = 0.001
train_batch = torch.utils.data.DataLoader(train_dataset_fashion, batch_size=batch_size, shuffle = True )
test_batch = torch.utils.data.DataLoader(test_dataset_fashion, batch_size=batch_size, shuffle = False )
loss_func = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model_fashion.parameters(), lr=learning_rate)
num_epochs = 15
model_fashion.to(device)
print("Training Fashion MNIST data")
train_model(model_fashion, train_batch, test_batch, num_epochs, loss_func, optimizer, device)
