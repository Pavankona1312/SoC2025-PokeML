# Week 2 - Neural Networks

## Task 1: Feedforward Neural Network and Backpropagation

- Implemented a 1-hidden-layer neural network using sigmoid activation and binary cross-entropy loss.
- Random weight initialization using normal distribution (mean 0, std 1).
- Performed forward and backward propagation.
- Trained using gradient descent over multiple epochs.
- Achieved perfect accuracy on the test set.

## Task 2: Neural Network for Image Classification

- Used a neural network with one hidden layer (sigmoid) and output layer (softmax).
- Trained using stochastic gradient descent.
- Implemented forward and backward propagation.
- Tracked cross-entropy loss on both training and validation sets.
- Predictions and metrics saved to output files.

## Task 3: Cats vs Dogs Classification

- Extended neural network to classify cats vs dogs.
- Used sigmoid activation at output layer for binary classification.
- Input: flattened grayscale image vectors.
- Normalized inputs between 0 and 1.
- Logged loss and predictions after training.

